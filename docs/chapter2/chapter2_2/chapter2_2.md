# Transformers 技术解析+实战(LLM)，多种 Transformers diffusion 模型技术图像生成技术+实战

<!-- 我是正文 -->

## Transformer+LLM

本小节，我们先回顾 SelfAttention，然后了解 LLaMA 架构。

### SelfAttention

关于 SelfAttention 的内容，可移步：[attention](./attention-llm/attention/attention.ipynb)

### LLaMA

关于 LLaMA 架构的介绍，可移步：[llm](./attention-llm/llm/llm.ipynb)

### 思考和练习

了解完相关知识，接下来才真正进入学习时刻。我们为读者准备了问题和练习，感兴趣的读者可以尝试去了解和实践。

具体可移步：[practice](./attention-llm/README.md)

### 录播回放地址

[录播回放](https://www.bilibili.com/video/BV17Z421a71d/)
