# Transformers技术解析+实战(LLM)，多种Transformers diffusion模型技术图像生成技术+实战
<!-- 我是正文 -->

## Transformer+LLM

本小节，我们先回顾SelfAttention，然后了解LLaMA架构。

### SelfAttention

关于SelfAttention的内容，可移步：[attention](./attention-llm/attention/attention.ipynb)

### LLaMA

关于LLaMA架构的介绍，可移步：[llm](./attention-llm/llm/llm.ipynb)

### 思考和练习

了解完相关知识，接下来才真正进入学习时刻。我们为读者准备了问题和练习，感兴趣的读者可以尝试去了解和实践。

具体可移步：[practice](./attention-llm/README.md)





